{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nurulsakinah/insurance-cross-selling-classification-xgb?scriptVersionId=194502458\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"Aim: build a model to predict whether the policyholders (customers) from past year will also be interested in Vehicle Insurance provided by the company.<br>\nEvaluation by ROC-AUC : needs to be maximized\n","metadata":{}},{"cell_type":"markdown","source":"# 1. Import libraries","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom xgboost.sklearn import XGBClassifier\nimport lightgbm as lgbm\nimport catboost as cb\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nimport optuna\n\n\n\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-10T14:51:23.440276Z","iopub.execute_input":"2024-07-10T14:51:23.441111Z","iopub.status.idle":"2024-07-10T14:51:25.473073Z","shell.execute_reply.started":"2024-07-10T14:51:23.441072Z","shell.execute_reply":"2024-07-10T14:51:25.471997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Load the dataset","metadata":{}},{"cell_type":"code","source":"# read file\ndf_ori = pd.read_csv('/kaggle/input/playground-series-s4e7/train.csv')\ndf_test = pd.read_csv('/kaggle/input/playground-series-s4e7/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:51:25.475111Z","iopub.execute_input":"2024-07-10T14:51:25.476179Z","iopub.status.idle":"2024-07-10T14:51:51.290309Z","shell.execute_reply.started":"2024-07-10T14:51:25.476137Z","shell.execute_reply":"2024-07-10T14:51:51.289323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Overview and understand the data\n","metadata":{}},{"cell_type":"code","source":"# get a brief look of the dataset\ndf_ori.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:51:51.291729Z","iopub.execute_input":"2024-07-10T14:51:51.292164Z","iopub.status.idle":"2024-07-10T14:51:51.313429Z","shell.execute_reply.started":"2024-07-10T14:51:51.292125Z","shell.execute_reply":"2024-07-10T14:51:51.312377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check the dataset size. The dataset is huge, over 11 million rows","metadata":{}},{"cell_type":"code","source":"# check shape of the data\ndf_ori.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:51:51.316454Z","iopub.execute_input":"2024-07-10T14:51:51.317116Z","iopub.status.idle":"2024-07-10T14:51:51.323699Z","shell.execute_reply.started":"2024-07-10T14:51:51.317083Z","shell.execute_reply":"2024-07-10T14:51:51.322524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check duplicate and missing values. <br>\nNo missing values or duplicates detected","metadata":{}},{"cell_type":"code","source":"df_ori.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:51:51.325014Z","iopub.execute_input":"2024-07-10T14:51:51.325409Z","iopub.status.idle":"2024-07-10T14:51:59.372776Z","shell.execute_reply.started":"2024-07-10T14:51:51.325374Z","shell.execute_reply":"2024-07-10T14:51:59.371623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check missing values\ndf_ori.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:51:59.374496Z","iopub.execute_input":"2024-07-10T14:51:59.37484Z","iopub.status.idle":"2024-07-10T14:52:01.048972Z","shell.execute_reply.started":"2024-07-10T14:51:59.37481Z","shell.execute_reply":"2024-07-10T14:52:01.047976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check data type\ndf_ori.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:01.050254Z","iopub.execute_input":"2024-07-10T14:52:01.050684Z","iopub.status.idle":"2024-07-10T14:52:01.061875Z","shell.execute_reply.started":"2024-07-10T14:52:01.050636Z","shell.execute_reply":"2024-07-10T14:52:01.060833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Since the dataset is huge, we can reduce the memory by changing the data type.<br>\nThis Memory Optimization Strategy was taken from \n<a href=\"https://www.kaggle.com/code/jmascacibar/optimizing-memory-usage-with-insurance-cross-sell?kernelSessionId=186392861\">this notebook</a> written by JMASCACIBAR.","metadata":{}},{"cell_type":"code","source":"def converting_datatypes(df):\n    '''This method reduces memory for numeric columns in the dataframe'''\n    df = df.copy()\n    try:\n        # Converting data types\n        df['Gender'] = df['Gender'].astype('category')\n        df['Vehicle_Age'] = df['Vehicle_Age'].astype('category')\n        df['Vehicle_Damage'] = df['Vehicle_Damage'].astype('category')\n        df['Age'] = df['Age'].astype('int8')\n        df['Driving_License'] = df['Driving_License'].astype('int8')\n        df['Region_Code'] = df['Region_Code'].astype('int8')\n        df['Previously_Insured'] = df['Previously_Insured'].astype('int8')\n        df['Annual_Premium'] = df['Annual_Premium'].astype('int32')\n        df['Policy_Sales_Channel'] = df['Policy_Sales_Channel'].astype('int16')\n        df['Vintage'] = df['Vintage'].astype('int16')\n        df['Response'] = df['Response'].astype('int8')\n        print(df.info(memory_usage='deep'))\n    except KeyError as e:\n        print(f\"Error: {e} not found in DataFrame\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:01.063251Z","iopub.execute_input":"2024-07-10T14:52:01.063667Z","iopub.status.idle":"2024-07-10T14:52:01.072946Z","shell.execute_reply.started":"2024-07-10T14:52:01.06363Z","shell.execute_reply":"2024-07-10T14:52:01.071828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"apply above function to the dataset","metadata":{}},{"cell_type":"code","source":"df = converting_datatypes(df_ori)\n# check the data size after converting","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:01.074333Z","iopub.execute_input":"2024-07-10T14:52:01.074749Z","iopub.status.idle":"2024-07-10T14:52:04.908731Z","shell.execute_reply.started":"2024-07-10T14:52:01.074711Z","shell.execute_reply":"2024-07-10T14:52:04.907598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Differentiate between cateorical and continous data so that we can plot appropriate graph for exploratory data analysis later. By looking at unique values, we can identify which is categorical features.","metadata":{}},{"cell_type":"code","source":"# check unique values in each column\nunique_col = df.nunique()\nunique_col","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:04.913372Z","iopub.execute_input":"2024-07-10T14:52:04.913784Z","iopub.status.idle":"2024-07-10T14:52:06.642837Z","shell.execute_reply.started":"2024-07-10T14:52:04.913757Z","shell.execute_reply":"2024-07-10T14:52:06.641778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Separate categorical & continuous variable","metadata":{}},{"cell_type":"code","source":"# get the categorical variables\n# Define minimum number of unique values threshold\nmin_unique = 54\n# get name columns\ncategorical_col = df.columns[df.nunique() <= min_unique].tolist()\ncont_col = df.columns[df.nunique() > min_unique].tolist()\n# drop 'id'\ncont_col.remove('id') \n\nprint(categorical_col)\nprint(cont_col)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:06.644627Z","iopub.execute_input":"2024-07-10T14:52:06.645053Z","iopub.status.idle":"2024-07-10T14:52:10.086107Z","shell.execute_reply.started":"2024-07-10T14:52:06.645016Z","shell.execute_reply":"2024-07-10T14:52:10.085038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode categorical features\nThe categorical data is encoded to numerical so that we can run correlation analysis on these features. ","metadata":{}},{"cell_type":"code","source":"def encode_categorical_features(df):    \n    df['Gender'] = df['Gender'].map({'Male': 0, 'Female': 1})\n    df['Vehicle_Age'] = df['Vehicle_Age'].map({'< 1 Year': 0, '1-2 Year': 1, '> 2 Years': 2})\n    df['Vehicle_Damage'] = df['Vehicle_Damage'].map({'No': 0, 'Yes': 1})\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:10.087851Z","iopub.execute_input":"2024-07-10T14:52:10.088289Z","iopub.status.idle":"2024-07-10T14:52:10.094331Z","shell.execute_reply.started":"2024-07-10T14:52:10.08825Z","shell.execute_reply":"2024-07-10T14:52:10.093274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = encode_categorical_features(df)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:10.095807Z","iopub.execute_input":"2024-07-10T14:52:10.09622Z","iopub.status.idle":"2024-07-10T14:52:10.140173Z","shell.execute_reply.started":"2024-07-10T14:52:10.096182Z","shell.execute_reply":"2024-07-10T14:52:10.139154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Exploratory Data Analysis\n\nExplore data by visualising each features.<br>\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Data distribution\nDistribution of target 'Response' distribution","metadata":{}},{"cell_type":"code","source":"# Checking if target data is Imbalanced\nresponse_data = df['Response'].value_counts()\n\nplt.figure(figsize=(4, 4))\nfig, ax = plt.subplots()\n\n# Add percentages to the pie chart\nax.pie(response_data, labels=[0, 1], autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff'])\nax.set_title(\"Piechart of 'Response' in data\", fontsize = 11)\nax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:10.141357Z","iopub.execute_input":"2024-07-10T14:52:10.141661Z","iopub.status.idle":"2024-07-10T14:52:10.355553Z","shell.execute_reply.started":"2024-07-10T14:52:10.141635Z","shell.execute_reply":"2024-07-10T14:52:10.35427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The piechart shows imbalanced data with Response '0' as majority class.","metadata":{}},{"cell_type":"markdown","source":"Distribution of continous variables","metadata":{}},{"cell_type":"code","source":"# Boxplot\n# Creating grid of subplots\nfig, ax = plt.subplots(2, 2, figsize=(13, 13))\n\nax = ax.flatten()\n# Loop through columns and plot box plots\nfor idx, col in enumerate(cont_col):\n    sns.boxplot(data=df, y=col, ax=ax[idx], color='skyblue')\n    ax[idx].set_title(f'Box Plot of {col}')\n    ax[idx].set_ylabel(col)\n    ax[idx].set_ylabel(col)\n\n# Adjust the layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:10.357113Z","iopub.execute_input":"2024-07-10T14:52:10.357932Z","iopub.status.idle":"2024-07-10T14:52:17.473094Z","shell.execute_reply.started":"2024-07-10T14:52:10.357882Z","shell.execute_reply":"2024-07-10T14:52:17.472051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a 2x2 grid of subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Flatten the axes array for easier iteration\naxes = axes.flatten()\n\n# Loop through columns and axes\nfor col, ax in zip(cont_col, axes):\n    sns.histplot(df[col], bins = 20,kde=True, ax=ax)\n    ax.set_title(f'Histogram and KDE of {col}', fontsize=12)\n    ax.set_xlabel(col, fontsize=10)\n    ax.set_ylabel('Density', fontsize=10)\n\n# Adjust the layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:52:17.474599Z","iopub.execute_input":"2024-07-10T14:52:17.47501Z","iopub.status.idle":"2024-07-10T14:55:44.623737Z","shell.execute_reply.started":"2024-07-10T14:52:17.474974Z","shell.execute_reply":"2024-07-10T14:55:44.622523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distribution for categorical variables","metadata":{}},{"cell_type":"code","source":"# Create a 2x2 grid of subplots\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Flatten the axes array for easier iteration\naxes = axes.flatten()\n\n# Loop through columns and axes\nfor col, ax in zip(categorical_col, axes):\n    sns.countplot(x=df[col], ax=ax, hue= df['Response'])\n    ax.set_title(f'Count Plot of {col}', fontsize=12)\n    ax.set_xlabel(col, fontsize=10)\n    ax.set_ylabel('Count', fontsize=10)\n\n# Adjust the layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:55:44.625454Z","iopub.execute_input":"2024-07-10T14:55:44.625885Z","iopub.status.idle":"2024-07-10T14:55:52.398023Z","shell.execute_reply.started":"2024-07-10T14:55:44.625846Z","shell.execute_reply":"2024-07-10T14:55:52.396803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Data shows most people have a driving license but at the same time, there are more response in 0 compared to 1.","metadata":{}},{"cell_type":"markdown","source":"### Correlation matrix","metadata":{}},{"cell_type":"code","source":"def correlation_analysis(df):\n    ''' visualise the correlation matrix,\n    and return the highest correlated pairs'''\n\n    # Correlation matrix\n    df_num = df.select_dtypes(include=[np.number, 'category'])\n\n    # Compute the correlation matrix\n    corr = df_num.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(15, 10))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask\n    sns.heatmap(corr, annot=True, mask=mask, cmap=cmap,\n                fmt='.2f', cbar=True, annot_kws={\"size\": 12})\n\n    # unstack the matrix\n    corr_unstacked = corr.unstack()\n\n    # Convert the Series to a DataFrame and reset the index\n    corr_df = pd.DataFrame(corr_unstacked).reset_index()\n    corr_df.columns = ['Feature1', 'Feature2', 'Correlation']\n\n    # Remove self-correlations (correlation of a feature with itself)\n    corr_df = corr_df[corr_df['Feature1'] != corr_df['Feature2']]\n\n    # Get the absolute values of the correlations\n    corr_df['Correlation'] = corr_df['Correlation'].abs()\n\n    # Sort the DataFrame by absolute correlation values in descending order\n    corr_df = corr_df.sort_values(by='Correlation', ascending=False)\n\n    corr_df['sorted_features'] = corr_df.apply(lambda row: tuple(\n        sorted([row['Feature1'], row['Feature2']])), axis=1)\n    corr_df = corr_df.drop_duplicates(subset=['sorted_features'])\n\n    # Extract the top 10 related features\n    top_related = corr_df.head(5)\n\n    return top_related\n\n\ntop_correlated = correlation_analysis(df)\n\ntop_correlated\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:55:52.399391Z","iopub.execute_input":"2024-07-10T14:55:52.399744Z","iopub.status.idle":"2024-07-10T14:55:59.673783Z","shell.execute_reply.started":"2024-07-10T14:55:52.399715Z","shell.execute_reply":"2024-07-10T14:55:59.672785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract feature pairs for visualization\ntop_pairs = top_correlated[['Feature1', 'Feature2']].values\ntop_pairs","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:55:59.675275Z","iopub.execute_input":"2024-07-10T14:55:59.675715Z","iopub.status.idle":"2024-07-10T14:55:59.684792Z","shell.execute_reply.started":"2024-07-10T14:55:59.675675Z","shell.execute_reply":"2024-07-10T14:55:59.683731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Model Building\nThe target is separated from the data and then the data is split into 80/20 ratio for train and test set","metadata":{}},{"cell_type":"code","source":"# separate the Response as y\ny = df['Response']\n# get X by dropping column id and 'Response' from df\nx = df.drop(columns=['id', 'Response'])\n\n# Split to train and test set by 80/20\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.20, random_state=0)\n\nprint(f'Training set size: {x_train.shape}, {y_train.shape}')\nprint(f'Testing set size: {x_test.shape}, {y_test.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:55:59.685936Z","iopub.execute_input":"2024-07-10T14:55:59.686232Z","iopub.status.idle":"2024-07-10T14:56:02.229404Z","shell.execute_reply.started":"2024-07-10T14:55:59.686206Z","shell.execute_reply":"2024-07-10T14:56:02.228355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost\nThe model was tuned using 10% of the data with Optuna.","metadata":{}},{"cell_type":"code","source":"# Hyperparameter tuning with optuna 2nd TEST\n\n# Extract 10% of the data\nx_sample, _, y_sample, _ = train_test_split(x, y, test_size=0.90, random_state=0)\n\n# Split the extracted 10% data into 80% train and 20% test sets\nx_train_s, x_test_s, y_train_s, y_test_s = train_test_split(x_sample, y_sample, test_size=0.20, random_state=0)\n\n# Verify the sizes of the splits\nprint(f'Training set size: {x_train_s.shape}, {y_train_s.shape}')\nprint(f'Testing set size: {x_test_s.shape}, {y_test_s.shape}')\n\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_loguniform('n_estimators', 7500, 15000),\n        'eta': trial.suggest_loguniform('eta', 0.01, 0.3),\n        'alpha': trial.suggest_loguniform('alpha', 0.01, 0.5),\n        'subsample': trial.suggest_uniform('subsample', 0.75, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.2, 0.5),\n        'max_depth': trial.suggest_int('max_depth', 10, 15),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 5),\n        'max_child_weight': trial.suggest_int('min_child_weight', 8, 12),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 1e-3),\n        'eval_metric': 'auc',\n        'random_state': 42,\n        'max_bin': trial.suggest_int('max_bin', 100000, 300000),\n        'tree_method':'hist',\n        'eval_metric':'auc',\n        'objective':'binary:logistic',\n        \"enable_categorical\": True  # Ensure categorical handling is enabled\n    }\n    \n    dtrain = xgb.DMatrix(x_train_s, label=y_train_s, enable_categorical=True)\n    dvalid = xgb.DMatrix(x_test_s, label=y_test_s, enable_categorical=True)\n    \n    watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n    \n    model = xgb.train(params, dtrain, evals=watchlist, early_stopping_rounds=50, verbose_eval=False)\n    \n    preds = model.predict(dvalid)\n    auc = roc_auc_score(y_test_s, preds)\n    \n    return auc\n\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50, timeout=600)\n\n\nprint(\"Best params: \", study.best_params)\nprint(\"Best AUC değeri: \", study.best_value)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T14:56:02.230881Z","iopub.execute_input":"2024-07-10T14:56:02.231273Z","iopub.status.idle":"2024-07-10T15:01:33.498729Z","shell.execute_reply.started":"2024-07-10T14:56:02.231236Z","shell.execute_reply":"2024-07-10T15:01:33.497628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# parameters from optuna, but change the n_estimators to 10000\nmodel = XGBClassifier(\n    n_estimators=10000,\n    eta= 0.28376548783458155,\n    alpha = 0.16464911888229491,\n    subsample = 0.8654931054006644, \n    colsample_bytree = 0.30702002596633265, \n    max_depth = 15,\n    min_child_weight = 5,\n    gamma = 0.0009847067724641178,\n    max_bin = 102824,\n    eval_metric='auc',\n    random_state=42,\n    enable_categorical=True\n)\n\n\n# Train the model with early stopping\nmodel.fit(\n    x_train,\n    y_train,\n    eval_set=[(x_test, y_test)],\n    early_stopping_rounds=50,\n    verbose=200\n)\n\n# Print the best iteration\nprint(\"Best iteration:\", model.best_iteration)\n\n# Use the underlying booster to predict on validation set using the best iteration\nbooster = model.get_booster()\ny_pred_prob = booster.predict(xgb.DMatrix(x_test, enable_categorical=True), iteration_range=(0, model.best_iteration + 1))\nauc = roc_auc_score(y_test, y_pred_prob)\nprint(f\"Validation AUC: {auc:.5f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T15:01:33.500114Z","iopub.execute_input":"2024-07-10T15:01:33.501356Z","iopub.status.idle":"2024-07-10T15:16:16.871712Z","shell.execute_reply.started":"2024-07-10T15:01:33.501321Z","shell.execute_reply":"2024-07-10T15:16:16.870532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"# 7. Model Evaluation \n","metadata":{}},{"cell_type":"code","source":"from yellowbrick.features import FeatureImportances\nfrom yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC, DiscriminationThreshold\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 15))\n\nmodel.importance_type = 'total_gain'\n\nvisualgrid = [\n    FeatureImportances(model,  ax=axes[0][0], colormap= 'winter'),\n    ConfusionMatrix(model, ax=axes[0][1], cmap= 'GnBu'),\n    ClassificationReport(model, ax=axes[1][0], cmap= 'GnBu'),\n    ROCAUC(model, ax=axes[1][1]),\n]\n\nfor viz in visualgrid:\n    viz.fit(x_train, y_train)\n    viz.score(x_test, y_test)\n    viz.finalize()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T15:16:16.883201Z","iopub.execute_input":"2024-07-10T15:16:16.883593Z","iopub.status.idle":"2024-07-10T15:20:39.043963Z","shell.execute_reply.started":"2024-07-10T15:16:16.883566Z","shell.execute_reply":"2024-07-10T15:20:39.042855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Submission","metadata":{}},{"cell_type":"code","source":"# # data preprocessing for subsmission\n# convert data type\ndf_test = converting_datatypes(df_test)\n# extraxt ids\ntest_ids = df_test['id']\n# remove id from dataset\ndf_test.drop(columns=['id'],inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T15:20:39.045599Z","iopub.execute_input":"2024-07-10T15:20:39.046306Z","iopub.status.idle":"2024-07-10T15:20:42.039634Z","shell.execute_reply.started":"2024-07-10T15:20:39.046247Z","shell.execute_reply":"2024-07-10T15:20:42.038785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\ndf_test_dmatrix = xgb.DMatrix(df_test, enable_categorical=True)\ny_pred = booster.predict(df_test_dmatrix,iteration_range=(0, model.best_iteration + 1))","metadata":{"execution":{"iopub.status.busy":"2024-07-10T15:20:42.040815Z","iopub.execute_input":"2024-07-10T15:20:42.041137Z","iopub.status.idle":"2024-07-10T15:22:44.249348Z","shell.execute_reply.started":"2024-07-10T15:20:42.041109Z","shell.execute_reply":"2024-07-10T15:22:44.248069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'Response': y_pred\n})\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-07-10T15:22:44.250908Z","iopub.execute_input":"2024-07-10T15:22:44.25125Z","iopub.status.idle":"2024-07-10T15:22:44.302186Z","shell.execute_reply.started":"2024-07-10T15:22:44.25122Z","shell.execute_reply":"2024-07-10T15:22:44.300818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T15:22:44.308232Z","iopub.execute_input":"2024-07-10T15:22:44.308717Z","iopub.status.idle":"2024-07-10T15:22:58.079693Z","shell.execute_reply.started":"2024-07-10T15:22:44.308685Z","shell.execute_reply":"2024-07-10T15:22:58.078756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 9. Conclusion\nMore improvements need to be done such as: <br>\nlook in depth into each features in EDA <br>\ntry to build LGBM model for comparison<br>","metadata":{}}]}