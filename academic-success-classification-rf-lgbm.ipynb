{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73290,"databundleVersionId":8710574,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nurulsakinah/academic-success-classification-rf-lgbm?scriptVersionId=194502360\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay, confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\nfrom scipy.stats import randint, uniform\nfrom sklearn import metrics\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import make_scorer,log_loss\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgbm\n\n\n\n\n# To ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T21:34:21.336182Z","iopub.execute_input":"2024-06-30T21:34:21.337442Z","iopub.status.idle":"2024-06-30T21:34:23.22316Z","shell.execute_reply.started":"2024-06-30T21:34:21.337407Z","shell.execute_reply":"2024-06-30T21:34:23.221985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the dataset","metadata":{}},{"cell_type":"code","source":"# read file\ndf = pd.read_csv('/kaggle/input/playground-series-s4e6/train.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:34:23.22523Z","iopub.execute_input":"2024-06-30T21:34:23.226135Z","iopub.status.idle":"2024-06-30T21:34:23.524454Z","shell.execute_reply.started":"2024-06-30T21:34:23.226097Z","shell.execute_reply":"2024-06-30T21:34:23.523106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning\ncheck missing data","metadata":{}},{"cell_type":"code","source":"# check missing values\ndf.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:34:23.526032Z","iopub.execute_input":"2024-06-30T21:34:23.526508Z","iopub.status.idle":"2024-06-30T21:34:23.549009Z","shell.execute_reply.started":"2024-06-30T21:34:23.52647Z","shell.execute_reply":"2024-06-30T21:34:23.54788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nTo get an overview of the data, we will check the unique values in each columns and the type of data\n","metadata":{}},{"cell_type":"code","source":"# calculate unique values in each column\nunique_col = df.nunique()\nprint(unique_col)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:34:23.552153Z","iopub.execute_input":"2024-06-30T21:34:23.552625Z","iopub.status.idle":"2024-06-30T21:34:23.5975Z","shell.execute_reply.started":"2024-06-30T21:34:23.552586Z","shell.execute_reply":"2024-06-30T21:34:23.596159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check type of data\nprint(df.info())","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:34:23.599201Z","iopub.execute_input":"2024-06-30T21:34:23.59966Z","iopub.status.idle":"2024-06-30T21:34:23.623751Z","shell.execute_reply.started":"2024-06-30T21:34:23.599624Z","shell.execute_reply":"2024-06-30T21:34:23.622625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Create a list to store the continuous and categorical variables","metadata":{}},{"cell_type":"code","source":"# get the categorical variables\ncategorical_col = df.select_dtypes(include='int').columns.tolist()\n# exclude Age at enrollment as that would be continous\ncategorical_col.remove('Age at enrollment')\nprint('Categorical variables: ', categorical_col)\nprint(len(categorical_col))\n\n\n# get continous variable\ncont_col = df.select_dtypes(exclude=['int', 'object']).columns.tolist()\ncont_col.append('Age at enrollment')\nprint('Continuous variables: ', cont_col)\nprint(len(cont_col))","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-30T21:34:23.625395Z","iopub.execute_input":"2024-06-30T21:34:23.625807Z","iopub.status.idle":"2024-06-30T21:34:23.641098Z","shell.execute_reply.started":"2024-06-30T21:34:23.625771Z","shell.execute_reply":"2024-06-30T21:34:23.639459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Visualisation\nIn this section, each columns will be explored and visualized to get an overview of the data distributions. <br>\nThe proportion of the 'Target' is plotted with countplot. <br>\n'Graduate' has the highest % ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Categorical variable\n# lets first check the categorical variable\n# Create a count plot for the 'Target' column\nplt.figure(figsize=(5, 3))\nsns.countplot(x='Target', data=df)\nplt.title('Count Plot of Target')\nplt.xlabel('Target')\nplt.ylabel('Count')\nplt.show()\n# the data is unbalanced\n\n# Calculate percentage of target\npercent_target = round((df['Target'].value_counts()/len(df)*100),2)\nprint(percent_target)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:34:23.643044Z","iopub.execute_input":"2024-06-30T21:34:23.64351Z","iopub.status.idle":"2024-06-30T21:34:24.046928Z","shell.execute_reply.started":"2024-06-30T21:34:23.643473Z","shell.execute_reply":"2024-06-30T21:34:24.045709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Boxplots\nBoxplot is used tovisualise the outliers and the mean of the continuous columns.<br>\nThe boxplots shows outliers exist in some of the variables. ","metadata":{}},{"cell_type":"code","source":"# lets do boxplot to identify outliers\n# Creating grid of subplots\nfig, ax = plt.subplots(3, 3, figsize=(9, 9))\n\nax = ax.flatten()\n# Loop through columns and plot box plots\nfor idx, col in enumerate(cont_col):\n    sns.boxplot(data=df, y=col, ax=ax[idx])\n    ax[idx].set_title(f'Box Plot of {col}', fontsize=10)\n    ax[idx].set_ylabel(col)\n\n# Hide the empty subplot\nif len(cont_col) < len(ax):\n    ax[len(cont_col)].set_visible(False)\n    \n# Adjust the layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:34:24.048281Z","iopub.execute_input":"2024-06-30T21:34:24.04862Z","iopub.status.idle":"2024-06-30T21:34:25.515016Z","shell.execute_reply.started":"2024-06-30T21:34:24.048594Z","shell.execute_reply":"2024-06-30T21:34:25.513535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Countplot and KDE plot\nCreate countplot for categorical variables and KDE plot for continuous variables.<br>","metadata":{}},{"cell_type":"code","source":"# Creating grid of subplots for distribution\nfig, axis = plt.subplots(12, 3, figsize=(20, 60))\n\n# Loop through columns and axes\nfor cols, ax in zip(df.drop(columns=['id', 'Target']).columns, axis.ravel()):\n    if cols in categorical_col:\n        # Get counts of each category and target combination\n        counts = df.groupby(\n            [cols, 'Target']).size().unstack(fill_value=0)\n\n        # Plot the stacked bar chart\n        counts.plot(kind='bar', stacked=True, ax=ax,\n                    color=sns.color_palette(\"pastel\"))\n        ax.set_ylabel('Count', fontsize=10)\n        ax.set_title(f'Categorical variable of {cols}', fontsize=10)\n    else:\n        # Plot hist and density plot for continuous feature\n        sns.kdeplot(data=df,\n                    x=df[cols], hue='Target', ax=ax, fill=True)\n        ax.set_ylabel('Density', fontsize=10)\n        ax.set_title(cols, fontsize=10)\n\n# Adjust the layout to prevent overlap\nplt.subplots_adjust(hspace=0.4)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:34:25.516794Z","iopub.execute_input":"2024-06-30T21:34:25.51724Z","iopub.status.idle":"2024-06-30T21:34:42.647969Z","shell.execute_reply.started":"2024-06-30T21:34:25.517201Z","shell.execute_reply":"2024-06-30T21:34:42.646394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_cat_percent(variable):\n    # Calculate the percentage of 1s in column\n    count_ones = df[variable].sum()\n    percentage_ones = round((count_ones /len(df['Educational special needs'])) * 100,2)\n    print(f'Percentage of 1 in column {variable} is :',percentage_ones,'%')\n    \ncount_cat_percent('Educational special needs')\ncount_cat_percent('International')\ncount_cat_percent('Debtor')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:34:42.651677Z","iopub.execute_input":"2024-06-30T21:34:42.652043Z","iopub.status.idle":"2024-06-30T21:34:42.659895Z","shell.execute_reply.started":"2024-06-30T21:34:42.652014Z","shell.execute_reply":"2024-06-30T21:34:42.658721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Educational special needs column looks like it has small category of 1. <br>\nCalculate the percentage of some of the columns with small proportion. <br>\nWe will drop the Educational special needs and International columns in later steps as these has very little values and does not provide much learning to the model","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing\n## Handling Outliers \n### Detect and Visualise the outliers\nThe outliers were calculated for the cont. columns & visualised with their respective upper and lower bound. <br>\nThe scatterplot of curricular units 1st sem(grade) & curricular units 1st sem(grade) shows 0 as outliers which is 'dropout' Target. Makes sense as if you have 0 grade, meaning you did not score well and grade is 0. <br>\nNot going to remove these  outliers as it could be a good learning for models","metadata":{}},{"cell_type":"code","source":"def outliers_analysis(df):\n    # save the percentage of outliers in a df\n    outliers_df = pd.DataFrame()\n\n    # calculate outliers\n    for i in (cont_col):\n        # Calculate IQR upper and lower limit\n        Q1 = df[i].quantile(0.25)\n        Q3 = df[i].quantile(0.75)\n        IQR = Q3 - Q1\n        lower = Q1 - 1.5 * IQR\n        upper = Q3 + 1.5 * IQR\n\n        # Identify outliers\n        outliers = df[(df[i] < lower) | (df[i] > upper)]\n        outliers_percent = round(len(outliers) / len(df) * 100, 2)\n        # Save outlier indices\n        outlier_indices = outliers.index.tolist()\n\n        # Create a new DataFrame with the current results\n        new_row = pd.DataFrame(\n            {'Column': [i],\n             'Outliers Percentage': [outliers_percent],\n             'Upper limit': [upper],\n             'Lower limit': [lower],\n             'Outlier index': [outlier_indices]})\n\n        # Append the new DataFrame to the existing DataFrame\n        outliers_df = pd.concat([outliers_df, new_row], ignore_index=True)\n\n        # Plot scatterplot of the outliers\n        plt.figure(figsize=(6, 2))\n        sns.scatterplot(x=df.index,\n                        y=df[i], hue=df['Target'], edgecolor='none', s=10)\n        plt.title(f'Scatter Plot of {i} vs Index', fontsize=10)\n        plt.xlabel('Index')\n        plt.ylabel(i)\n        plt.legend(title='Target')\n        # plot lower and upper bound\n        plt.axhline(lower, color='red', linestyle='--')\n        plt.axhline(upper, color='red', linestyle='--')\n        plt.show()\n\n    # Sorting by column 'Country'\n    outliers_df.sort_values(\n        by=['Outliers Percentage'], ascending=True, inplace=True)\n\n    # Plot outliers percentage by colummn\n    plt.figure(figsize=(6, 2))\n    sns.barplot(data=outliers_df, x='Column',\n                y='Outliers Percentage', palette='viridis')\n\n    # Add title and labels\n    plt.title('Percentage of Outliers per Column', fontsize=10)\n    plt.xlabel('Column')\n    plt.ylabel('Outliers in Percentage')\n\n    # Rotate x-axis labels for better readability if needed\n    plt.xticks(rotation=90)\n\n    # Display the plot\n    plt.tight_layout()\n    plt.show()\n\n    return outliers_df\n\n\noutliers_df = outliers_analysis(df)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:34:42.661678Z","iopub.execute_input":"2024-06-30T21:34:42.662129Z","iopub.status.idle":"2024-06-30T21:35:08.150677Z","shell.execute_reply.started":"2024-06-30T21:34:42.662092Z","shell.execute_reply":"2024-06-30T21:35:08.14939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Outliers removal\nThe outliers in column 'Previous qualification (grade)','Admission grade' is removed.","metadata":{}},{"cell_type":"code","source":"# Remove outliers\ndef outliers_removal(df, outliers_data, selected_column):\n    \n    # Filter the DataFrame to include only the desired rows\n    filtered_df = outliers_data[outliers_data['Column'].isin(selected_column)]\n    \n    # Combine the lists in the 'outlier index' column into a single set\n    outlier_indices = set()\n    for i in filtered_df['Outlier index']:\n        outlier_indices.update(i)\n        \n    # Remove outliers from the original DataFrame\n    df_cleaned = df.drop(index=outlier_indices)\n    \n    return df_cleaned\n\n# specify which column with outliers to be removed\nselected_column = ['Previous qualification (grade)',\n                   'Admission grade']\n\n# create a cleaned df\ndf_cleaned = outliers_removal(df, outliers_df, selected_column)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:35:08.152253Z","iopub.execute_input":"2024-06-30T21:35:08.152719Z","iopub.status.idle":"2024-06-30T21:35:08.178774Z","shell.execute_reply.started":"2024-06-30T21:35:08.152681Z","shell.execute_reply":"2024-06-30T21:35:08.177106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation Analysis\nCreate correlation matrix to see if if there is multicollinearity between the variables. ","metadata":{}},{"cell_type":"code","source":"def correlation_analysis(df):\n    ''' visualise the correlation matrix,\n    and return the highest correlated pairs'''\n\n    # Correlation matrix\n    df_num = df.select_dtypes(include=[np.number])\n\n    # Compute the correlation matrix\n    corr = df_num.corr()\n\n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(20, 15))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask\n    sns.heatmap(corr, annot=True, mask=mask, cmap=cmap,\n                fmt='.2f', cbar=True, annot_kws={\"size\": 8})\n\n    # unstack the matrix\n    corr_unstacked = corr.unstack()\n\n    # Convert the Series to a DataFrame and reset the index\n    corr_df = pd.DataFrame(corr_unstacked).reset_index()\n    corr_df.columns = ['Feature1', 'Feature2', 'Correlation']\n\n    # Remove self-correlations (correlation of a feature with itself)\n    corr_df = corr_df[corr_df['Feature1'] != corr_df['Feature2']]\n\n    # Get the absolute values of the correlations\n    corr_df['Correlation'] = corr_df['Correlation'].abs()\n\n    # Sort the DataFrame by absolute correlation values in descending order\n    corr_df = corr_df.sort_values(by='Correlation', ascending=False)\n\n    corr_df['sorted_features'] = corr_df.apply(lambda row: tuple(\n        sorted([row['Feature1'], row['Feature2']])), axis=1)\n    corr_df = corr_df.drop_duplicates(subset=['sorted_features'])\n\n    # Extract the top 10 related features\n    top_related = corr_df.head(10)\n\n    return top_related\n\n\n# top_correlated = correlation_analysis(df)\ntop_correlated = correlation_analysis(df_cleaned)\n\nprint('Top related features: ', top_correlated)\n\n# Extract feature pairs for visualization\ntop_pairs = top_correlated[['Feature1', 'Feature2']].values\n\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:35:08.180071Z","iopub.execute_input":"2024-06-30T21:35:08.180457Z","iopub.status.idle":"2024-06-30T21:35:11.489937Z","shell.execute_reply.started":"2024-06-30T21:35:08.180426Z","shell.execute_reply":"2024-06-30T21:35:11.488681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the subplots grid\nfig, ax = plt.subplots(5, 2, figsize=(15, 15))  # Adjusted figsize for better visibility\nax = ax.flatten()  # Flatten in case of multi-dimensional axes array\n\n# Loop through each pair and create a scatter plot in the corresponding subplot\nfor i, pair in enumerate(top_pairs):\n    feature1, feature2 = pair\n    sns.scatterplot(data=df, x=feature1, y=feature2, ax=ax[i])\n    ax[i].set_title(f'{feature1} vs {feature2}')\n\n\n# Adjust the layout to prevent overlap\nplt.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:35:11.491674Z","iopub.execute_input":"2024-06-30T21:35:11.492031Z","iopub.status.idle":"2024-06-30T21:35:15.416263Z","shell.execute_reply.started":"2024-06-30T21:35:11.492002Z","shell.execute_reply":"2024-06-30T21:35:15.414882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature selection\nFeatures to drop: \nCurricular units 1st sem(enrolled) <br>\nCurricular units 1st sem (credited) <br> \nCurricular units 1st sem (approved) <br>\nNacionality - we can distinguish this by international features. <br>\nThese are the features that has linear relationship based on the graph. <br>\nI've decided to keep the 2nd sem as it seems more reasonable to take the latest result compared to 1st semester\n\n\n","metadata":{}},{"cell_type":"code","source":"columns_drop = ['Curricular units 1st sem (enrolled)',\n                'Curricular units 1st sem (credited)',\n                'Curricular units 1st sem (approved)',\n                'Nacionality',\n               'Educational special needs']\ndf_cleaned.drop(columns_drop, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:35:15.417683Z","iopub.execute_input":"2024-06-30T21:35:15.418019Z","iopub.status.idle":"2024-06-30T21:35:15.431853Z","shell.execute_reply.started":"2024-06-30T21:35:15.417989Z","shell.execute_reply":"2024-06-30T21:35:15.43044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building\n## Split the data\nData is split into 75/25 ratio for train and test","metadata":{}},{"cell_type":"code","source":"# encode the target variable\ndf_cleaned['Target'] = df_cleaned['Target'].map(\n    {'Graduate': 2, 'Enrolled': 1, 'Dropout': 0})\n\n# separate the Target as y\ny = df_cleaned['Target']\n# get X by dropping column id and Target from df\nx = df_cleaned.drop(columns=['id', 'Target'])\n\n# Split to train and test set by 75/25\nx_train, x_test, y_train, y_test = train_test_split(\n    x, y, test_size=0.25, random_state=0)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:35:15.433239Z","iopub.execute_input":"2024-06-30T21:35:15.433712Z","iopub.status.idle":"2024-06-30T21:35:15.483542Z","shell.execute_reply.started":"2024-06-30T21:35:15.433681Z","shell.execute_reply":"2024-06-30T21:35:15.482338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest Model\n### Hyperparameter tuning\nHyperparameter tuning to get the best parameter. <br>\nNotes: tuning was done with RandomizedSearchCV, it takes a while so i will comment the code and set the best parameter found to variable 'rf_model'. <br>","metadata":{}},{"cell_type":"code","source":"# # Define the parameter grid\n# param_dist = {\n#     'n_estimators': randint(100, 1000),  # Number of trees in the forest\n#     'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider at every split\n#     'max_depth': randint(10, 100),  # Maximum number of levels in the tree\n#     'min_samples_split': randint(2, 20),  # Minimum number of samples required to split a node\n#     'min_samples_leaf': randint(1, 20),  # Minimum number of samples required at each leaf node\n#     'bootstrap': [True, False]  # Method of selecting samples for training each tree\n# }\n\n# # Initialize RandomizedSearchCV\n# rf_search = RandomizedSearchCV(\n#     estimator=rf, \n#     param_distributions=param_dist, \n#     n_iter=100,  # Number of parameter settings that are sampled\n#     cv=3,  # 3-fold cross validation\n#     verbose=2, \n#     random_state=42,\n#     n_jobs=-1  # Use all available cores\n# )\n\n# # Fit the random search model\n# rf_search.fit(x_train, y_train)\n\n# # Create a variable for the best model\n# best_rf = rf_search.best_estimator_\n\nrf_model = RandomForestClassifier(\n    bootstrap= False, \n    max_depth= 84, \n    max_features= 'auto', \n    min_samples_leaf= 4, \n    min_samples_split= 5, \n    n_estimators= 493)\n\n# Fit the model on the training data\nrf_model.fit(x_train, y_train)\n\n# Predict on the test data\ny_pred_rf = rf_model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:35:15.485018Z","iopub.execute_input":"2024-06-30T21:35:15.485542Z","iopub.status.idle":"2024-06-30T21:36:42.780293Z","shell.execute_reply.started":"2024-06-30T21:35:15.485509Z","shell.execute_reply":"2024-06-30T21:36:42.779029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Importance","metadata":{}},{"cell_type":"code","source":"# check feature importances\nfeature_importances = pd.Series(rf_model.feature_importances_, index=x_train.columns)\nfeature_importances.sort_values(ascending=True, inplace=True)\nfeature_importances.plot.barh(color='green')\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Feature\")\nplt.title(\"Feature Importance\", fontsize=10)\nplt.xticks(fontsize=7)\nplt.yticks(fontsize=7)\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:42.781786Z","iopub.execute_input":"2024-06-30T21:36:42.782127Z","iopub.status.idle":"2024-06-30T21:36:43.374619Z","shell.execute_reply.started":"2024-06-30T21:36:42.782099Z","shell.execute_reply":"2024-06-30T21:36:43.373539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM Classifier\n### Hyperparameter tuning\nI feel that the hyperparameter tuning here can be improved, but due to limited time, this will be used for now","metadata":{}},{"cell_type":"code","source":"# lgbm_params = {\n#                  'objective': 'multiclass',\n#                  'tree_learner': 'feature', \n#                  'n_estimators': randint(100, 1000), \n#                  'num_leaves': randint(100, 1000),\n#                  'max_depth': [10, 20, 30, 40, 50],\n#                  'verbose': -1, \n#                  'random_state': 42 \n# }\n\n# # Create the LGBMClassifier for multi-class classification\n# lgbm_model = lgbm.LGBMClassifier(objective=\"multiclass\", num_class=3)\n\n# # Perform RandomizedSearchCV with StratifiedKFold\n# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n# search = RandomizedSearchCV(lgbm_model, param_distributions=param_grid, n_iter=20, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1, random_state=42)\n# search.fit(x_train, y_train)\n\n# # Print best parameters and best score\n# print(\"Best CV score: {:.4f}\".format(search.best_score_))\n# print(\"Best parameters:\", search.best_params_)\n\n# # Get the best model\n# best_model = search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:43.375968Z","iopub.execute_input":"2024-06-30T21:36:43.376299Z","iopub.status.idle":"2024-06-30T21:36:43.381742Z","shell.execute_reply.started":"2024-06-30T21:36:43.376272Z","shell.execute_reply":"2024-06-30T21:36:43.380613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build the lightgbm model based on best_model parameter from hyperparameter tuning \nlgbm_model = LGBMClassifier(max_depth= 50,\n                            num_class=3, \n                            objective='multiclass',\n                            preprocessor__cat__imputer__strategy='most_frequent')\nlgbm_model.fit(x_train, y_train)\n\n# make prediction\ny_pred_lgbm=lgbm_model.predict(x_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:43.383141Z","iopub.execute_input":"2024-06-30T21:36:43.383545Z","iopub.status.idle":"2024-06-30T21:36:47.228845Z","shell.execute_reply.started":"2024-06-30T21:36:43.383516Z","shell.execute_reply":"2024-06-30T21:36:47.227508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation\n## Accuracies\n","metadata":{}},{"cell_type":"code","source":"def evaluate_model(prediction):\n    ''' # function for evaluation matrix '''\n    # Create the confusion matrix\n    cm = confusion_matrix(y_test, prediction)\n    ConfusionMatrixDisplay(confusion_matrix=cm).plot()\n    accuracy = round(accuracy_score(y_test, prediction), 5)\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:47.23089Z","iopub.execute_input":"2024-06-30T21:36:47.231237Z","iopub.status.idle":"2024-06-30T21:36:47.236742Z","shell.execute_reply.started":"2024-06-30T21:36:47.231208Z","shell.execute_reply":"2024-06-30T21:36:47.235597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_accuracy = evaluate_model(y_pred_rf)\n\nprint('Random Forest accuracy: ',rf_accuracy)\nprint(classification_report(y_test, y_pred_rf))\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:47.238118Z","iopub.execute_input":"2024-06-30T21:36:47.238484Z","iopub.status.idle":"2024-06-30T21:36:47.608297Z","shell.execute_reply.started":"2024-06-30T21:36:47.238455Z","shell.execute_reply":"2024-06-30T21:36:47.606853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_accuracy = evaluate_model(y_pred_lgbm)\n\nprint('LightGBM accuracy: ',lgbm_accuracy)\nprint(classification_report(y_test, y_pred_lgbm))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:47.609755Z","iopub.execute_input":"2024-06-30T21:36:47.61009Z","iopub.status.idle":"2024-06-30T21:36:47.963812Z","shell.execute_reply.started":"2024-06-30T21:36:47.61006Z","shell.execute_reply":"2024-06-30T21:36:47.962593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Check for overfitting\nThe training accuracy for Random Forest model is 0.9462 which we can say is a bit overfitting the data compared to the LightGBM model","metadata":{}},{"cell_type":"code","source":"print('Random Forest Training Accuracy : {:.4f}'.format(metrics.accuracy_score(y_train, rf_model.predict(x_train))))\nprint('LightGBM Training Accuracy : {:.4f}'.format(metrics.accuracy_score(y_train, lgbm_model.predict(x_train))))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:47.965058Z","iopub.execute_input":"2024-06-30T21:36:47.965406Z","iopub.status.idle":"2024-06-30T21:36:56.826955Z","shell.execute_reply.started":"2024-06-30T21:36:47.965378Z","shell.execute_reply":"2024-06-30T21:36:56.825758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implement model on submission data\n","metadata":{}},{"cell_type":"code","source":"df_test  = pd.read_csv('/kaggle/input/playground-series-s4e6/test.csv')\n# extraxt ids\ntest_ids = df_test['id']\ndf_test.drop(columns=['id'],inplace = True)\ndf_test.drop(columns_drop, axis=1, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:56.828395Z","iopub.execute_input":"2024-06-30T21:36:56.828807Z","iopub.status.idle":"2024-06-30T21:36:56.994209Z","shell.execute_reply.started":"2024-06-30T21:36:56.828777Z","shell.execute_reply":"2024-06-30T21:36:56.993031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\ny_pred = lgbm_model.predict(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:56.995573Z","iopub.execute_input":"2024-06-30T21:36:56.995919Z","iopub.status.idle":"2024-06-30T21:36:58.086184Z","shell.execute_reply.started":"2024-06-30T21:36:56.995889Z","shell.execute_reply":"2024-06-30T21:36:58.084799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create submission file\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'Target': y_pred\n})\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:58.08812Z","iopub.execute_input":"2024-06-30T21:36:58.088502Z","iopub.status.idle":"2024-06-30T21:36:58.104688Z","shell.execute_reply.started":"2024-06-30T21:36:58.088474Z","shell.execute_reply":"2024-06-30T21:36:58.1034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the reverse mapping to the 'Target' column\nsubmission['Target'] = submission['Target'].map({2: 'Graduate', 1: 'Enrolled', 0: 'Dropout'})\nsubmission","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:58.111067Z","iopub.execute_input":"2024-06-30T21:36:58.111563Z","iopub.status.idle":"2024-06-30T21:36:58.129598Z","shell.execute_reply.started":"2024-06-30T21:36:58.111523Z","shell.execute_reply":"2024-06-30T21:36:58.128375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:36:58.131431Z","iopub.execute_input":"2024-06-30T21:36:58.131871Z","iopub.status.idle":"2024-06-30T21:36:58.196559Z","shell.execute_reply.started":"2024-06-30T21:36:58.131832Z","shell.execute_reply":"2024-06-30T21:36:58.195275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nThere is definitely a lot of areas for improvements especially in the hyperparameter tuning for LGBM model. <br>\nI would also consider building a pipeline in future projects <br>\nMore explanations should be provided during EDA based on the plots as well","metadata":{}}]}